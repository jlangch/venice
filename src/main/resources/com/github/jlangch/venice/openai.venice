;;;;   __    __         _
;;;;   \ \  / /__ _ __ (_) ___ ___
;;;;    \ \/ / _ \ '_ \| |/ __/ _ \
;;;;     \  /  __/ | | | | (_|  __/
;;;;      \/ \___|_| |_|_|\___\___|
;;;;
;;;;
;;;; Copyright 2017-2024 Venice
;;;;
;;;; Licensed under the Apache License, Version 2.0 (the "License");
;;;; you may not use this file except in compliance with the License.
;;;; You may obtain a copy of the License at
;;;;
;;;;     http://www.apache.org/licenses/LICENSE-2.0
;;;;
;;;; Unless required by applicable law or agreed to in writing, software
;;;; distributed under the License is distributed on an "AS IS" BASIS,
;;;; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
;;;; See the License for the specific language governing permissions and
;;;; limitations under the License.

;;;; OpenAI client without any dependencies on 3rd party libraries


(ns openai)

(load-module :http-client-j8 ['http-client-j8 :as 'hc])
(load-module :ansi)


;; =============================================================================
;; =                                                                           =
;; =                     W O R K   I N   P R O G R E S S                       =
;; =                                                                           =
;; =============================================================================


;; -----------------------------------------------------------------------------
;; MODELS 
;; -----------------------------------------------------------------------------

;; see https://platform.openai.com/docs/models/continuous-model-upgrades

(def gpt-4-turbo   "gpt-4.0-turbo")    ;; GPT-4 Turbo with Vision, 128'000 tokens
(def gpt-4         "gpt-4")            ;; GPT-4, 8'192 tokens
(def gpt-4-32k     "gpt-4-32k")        ;; GPT-4, 32'768 tokens
(def gpt-3.5-turbo "gpt-3.5-turbo")    ;; GPT-3.5, 16'385 tokens

(def dall-e-3      "dall-e-3")         ;; DALL-E 3
(def dall-e-2      "dall-e-2")         ;; DALL-E 2

(def tts-1         "tts-1")            ;; Text-to-speech 1
(def tts-1-hd      "tts-1-hd")         ;; Text-to-speech 1 HD

(def whisper-1     "whisper-1")        ;; Whisper, general-purpose speech recognition

(def text-embedding-3-large  "text-embedding-3-large")   ;; Text embedding, dimension 3'072
(def text-embedding-3-small  "text-embedding-3-small")   ;; Text embedding, dimension 1'536
(def text-embedding-ada-002  "text-embedding-ada-002")   ;; Text embedding, dimension 1'536

(def default-model "gpt-3.5-turbo")




;; -----------------------------------------------------------------------------
;; CHAT COMPLETIONS 
;; -----------------------------------------------------------------------------

;; https://platform.openai.com/docs/guides/text-generation
;; https://platform.openai.com/docs/api-reference/chat/create

(def endpoint-gpt-new "https://api.openai.com/v1/chat/completions")
(def endpoint-gpt-old "https://api.openai.com/v1/completions")


(defn 
  ^{ :arglists '(
          "(chat-completion prompt & options)" )
     :doc """
          Runs a chat completion.

          To run the request asynchronously just wrap it in a `future` and
          deref it, when the result is required.

          ¶*Parameter «prompt»*

          A prompt is either a simple string like:

          ```
          "Who won the world series in 2020?"
          ```

          or a list of prompt messages:

          ```
          [ {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Who won the world series in 2020?"},
            {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
            {"role": "user", "content": "Where was it played?"} ]
          ```

          Using prompt roles:

          | [![width: 15%]] | [![width: 85%]] |
          | *system*    | Allows to specify the way the model answers questions. ¶\
                          Classic example: "You are a helpful assistant." |
          | *user*      | Equivalent to the queries made by the user. |
          | *assistant* | Assistent roles are the model’s responses, based on the user messages. |


          ¶*Parameter request «options»*

          | [![width: 15%]]   | [![width: 85%]] |
          | :uri              | An OpenAI chat completion URI. E.g.: \
                                "https://api.openai.com/v1/chat/completions". ¶\
                                Defaults  to "https://api.openai.com/v1/chat/completions" |
          | :model            | An OpenAI model. E.g.: "gpt-3.5-turbo". Defaults \
                                to "gpt-3.5-turbo" |
          | :openai-api-key   | An optional OpenAI API Key. As default the key is read \
                                from the environment variable "OPENAI_API_KEY". |
          | :prompt-opts      | An optional map of OpenAI chat request prompt options ¶\
                                E.g. `{:temperature 0.2}` ¶\
                                See: [OpenAI Request Options](https://platform.openai.com/docs/api-reference/chat/create) |
          | :tools            | a list of tools. e.g.: function definitions (see OpenAI api for details) |
          | :tool-choice      | a tool choice. e.g.: function definitions (see OpenAI api for details) ¶\
                                This forces the model to use a specific function: ¶\
                                `{:type "function", :function {:name "get_n_day_weather_forecast"}` |
          | :debug            | An optional debug flag (true/false). Defaults \
                                to false. ¶\
                                In debug mode prints the HTTP request and response data |

          Tools options (a Venice map) for passing a function definition:

          ```
          [ 
            { :type "function" 
              :function {
                  :name "get_current_weather"
                  :description "Get the current weather"
                  :parameters {
                      :type "object"
                      :properties {
                          :location {
                              :type "string"
                              :description "The city and state, e.g. San Francisco, CA"
                          }
                          :format {
                              :type "string"
                              :enum ["celsius", "fahrenheit"]
                              :description "The temperature unit to use. Infer this from the users location."
                          }
                      }
                      :required ["location", "format"]
                  } 
              } 
            }
          ]
          ```

          ¶*Return value*

          Returns a map with the response data:*

          | [![width: 15%]] | [![width: 85%]]                  |
          | :status         | The HTTP status (a long)         |
          | :mimetype       | The content type's mimetype      |
          | :headers        | A map of headers. key: header name, value: list \
                              of header values |
          | :message        | The final chat completion message if the OpenAI \
                              server returned the HTTP status `HTTP_OK`, else `nil` |
          | :data           | If the response' HTTP status is `HTTP_OK` \
                              the data fields contains the chat completion \
                              message. ¶\
                              If the response' HTTP status is not `HTTP_OK` \
                              the data fields contains an error message \
                              formatted as plain or JSON string. |
          
          ¶
          See:
          * [OpenAI Chat Completions API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)¶
          * [OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat/create)
          * [OpenAI API Messages](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)
          * [OpenAI API Functions](https://platform.openai.com/docs/guides/function-calling)
          * [OpenAI API Functions Cookbook](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models)
          * [OpenAI API Examples](https://platform.openai.com/examples)
          * [OpenAI API Examples Prompts](https://platform.openai.com/examples?category=code)
          """ 
     :examples '(
          """
          ;; print the full OpenAI response message
          (do
            (load-module :openai)

            (let [prompt    (str "Count to 10, with a comma between each number "
                                 "and no newlines. E.g., 1, 2, 3, ...")
                  response  (openai/chat-completion prompt)]
              (println "Status:  " (:status response))
              (println "Mimetype:" (:mimetype response))
              (if (=  (:status response) 200)
                (println "Message:" (openai/pretty-print-json (:data response)))
                (println "Error:"   (:data response)))))
          """,
          """
          ;; print only the OpenAI response message content
          (do
            (load-module :openai)

            (let [prompt    (str "Count to 10, with a comma between each number "
                                 "and no newlines. E.g., 1, 2, 3, ...")
                  response  (openai/chat-completion prompt)]
              (println "Status:  " (:status response))
              (println "Mimetype:" (:mimetype response))
              (if (=  (:status response) 200)
                (println "Message:" (-> (:data response)
                                        (openai/extract-response-message-content)
                                        (pr-str)))
                (println "Error:"   (:data response)))))
          """,
          """
          ;; Dealing with prompt options
          (do
            (load-module :openai)

            (let [prompt   [ { :role     "system"
                               :content  "You will be provided with statements, and your task is to convert them to standard English." }
                             { :role     "user"
                               :content  "She no went to the market." } ]
                  prompt-opts { :temperature 0.7
                                :max_tokens 64
                                :top_p 1 }
                  response  (openai/chat-completion prompt 
                                                    :model "gpt-3.5-turbo" 
                                                    :prompt-opts prompt-opts)]
              (println "Status:  " (:status response))
              (println "Mimetype:" (:mimetype response))
              (if (=  (:status response) 200)
                (println "Message:" (-> (:data response)
                                        (openai/extract-response-message-content)
                                        (openai/pretty-print-json)))
                (println "Error:"   (:data response)))))
            """ )
     :see-also '( 
          "openai/chat-completion-streaming"
          "openai/extract-response-message-content"
          "openai/pretty-print-json" ) }

  chat-completion [prompt & options]

  (let [opts            (apply hash-map options)
        _               (assert (or (nil? (:prompt-opts opts)) (map? (:prompt-opts opts))))
        uri             (:uri opts "https://api.openai.com/v1/chat/completions")
        model           (:model opts default-model)
        openai-api-key  (or (:openai-api-key opts) (openai-api-key-from-env))
        _               (validate-openai-api-key openai-api-key)
        tools           (:tools opts)
        _               (validate-tools tools)
        tool-choice     (:tool_choice opts)
        _               (validate-tool-choice tool-choice)
        body            (build-prompt model prompt (:prompt-opts opts)
                                      tools tool-choice{:stream false})
        debug?          (:debug opts false)
        _               (when debug? (dump-prompt body))
        response        (hc/send :post 
                          uri
                          :headers { "Content-Type" "application/json"
                                     "Authorization" (str "Bearer " openai-api-key)}
                          :body (json/write-str body)
                          :debug  debug?)
        result          { :status   (:http-status response)
                          :mimetype (:content-type-mimetype response)
                          :headers  (:headers response) } ]
   (assoc result :data  (hc/slurp-response response 
                                           :json-parse-mode :data 
                                           :json-key-fn keyword))))


(defn 
 ^{ :arglists '(
          "(chat-completion-streaming prompt handler & options)" )
     :doc """
          Runs a chat completion in streaming mode.

          Processes OpenAI server side events (SSE) and calls for every event the
          handler 'handler'.


          ¶*Parameter «prompt»*

          A prompt is either a simple string like:

          ```
          "Who won the world series in 2020?"
          ```

          or a list of prompt messages:

          ```
          [ {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Who won the world series in 2020?"},
            {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
            {"role": "user", "content": "Where was it played?"} ]
          ```

          ¶*Parameter «handler»*
       
          The event handler is a three argument function:
          
          ```
          (defn handler [delta accumulated status] ...)
          ```
         
          Handler arguments:

          | [![width: 15%]] | [![width: 85%]]                  |
          | *delta*       | the delta message sent with the event |
          | *accumulated* | the accumulated message|
          | *type*        | the notification type: ¶\
                            \u00A0\u00A0 `:opened` - streaming started ¶\
                            \u00A0\u00A0 `:data` - streamed event ¶\
                            \u00A0\u00A0 `:done` - streaming done by the server |

          ¶*Parameter request «options»*
 
          | [![width: 15%]]   | [![width: 85%]] |
          | :uri              | An OpenAI chat completion URI. E.g.: \
                                "https://api.openai.com/v1/chat/completions". ¶\
                                Defaults  to "https://api.openai.com/v1/chat/completions" |
          | :model            | An OpenAI model. E.g.: "gpt-3.5-turbo". Defaults \
                                to "gpt-3.5-turbo" |
          | :openai-api-key   | An optional OpenAI API Key. As default the key is read \
                                from the environment variable "OPENAI_API_KEY". |
          | :sync             | if *true* runs the request syncronously and waits \
                                until the full message response is available. ¶\
                                if *false* runs the request asyncronously and \
                                returns immediately with the response :data \
                                field holding a `future` that can be deref'd  \
                                (with an optional timeout) to get the full \
                                message. ¶\
                                Defaults to *true* |
          | :prompt-opts      | An optional map of OpenAI chat request prompt options ¶\
                                E.g. {:temperature 0.2} ¶\
                                See: [OpenAI Request Options](https://platform.openai.com/docs/api-reference/chat/create) |
          | :debug            | An optional debug flag (true/false). Defaults \
                                to false. ¶\
                                In debug mode prints the HTTP request and response data |
 
          ¶*Return value*

          Returns a map with the response data:

          | [![width: 15%]] | [![width: 85%]]                  |
          | :status         | The HTTP status (a long)         |
          | :mimetype       | The content type's mimetype      |
          | :headers        | A map of headers. key: header name, value: list \
                              of header values |
          | :message        | The final chat completion message if the OpenAI \
                              server returned the HTTP status `HTTP_OK`, else `nil` |
          | :data           | If the response' HTTP status is `HTTP_OK` \
                              the data fields contains the chat completion \
                              message.  ¶\
                              If the response' HTTP status is not `HTTP_OK` \
                              the data fields contains an error message \
                              formatted as plain or JSON string. |
          
          ¶*Note: The streaming mode does not support functions!*
 
          See:
          * [OpenAI Chat Completions API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)¶
          * [OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat/create)
          * [OpenAI API Messages](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)
          * [OpenAI API Examples](https://platform.openai.com/examples)
          * [OpenAI API Examples Prompts](https://platform.openai.com/examples?category=code)
          """ 
     :examples '(
          """
          ;; synchronous
          ;; prints the arriving events asynchronously, the response is only
          ;; returned when the final message is available or the request is bad
          (do
            (load-module :openai)
          
            (let [prompt    (str "Count to 10, with a comma between each number "
                                 "and no newlines. E.g., 1, 2, 3, ...")
                  handler   (fn [delta accumulated status]
                              (case status
                                :opened  (println "Started...")
                                :data    (println "Delta:" (pr-str delta))
                                :done    (println "Completed.")))
                  response  (openai/chat-completion-streaming prompt handler :sync true)]
              (println "Status:  " (:status response))
              (println "Mimetype:" (:mimetype response))
              (if (=  (:status response) 200)
                (println "Message:" (pr-str (:data response)))
                (println "Error:"   (:data response)))))
          """,
          """
          ;; asynchronous
          ;; prints the arriving events asynchronously, returns the response
          ;; immediately with the data  `(:data response)` as a future that can 
          ;; be deref'd to get the final message.
          (do
            (load-module :openai)
          
            (let [prompt    (str "Count to 10, with a comma between each number "
                                 "and no newlines. E.g., 1, 2, 3, ...")
                  handler   (fn [delta accumulated status]
                              (case status
                                :opened  (println "Started...")
                                :data    (println "Delta:" (pr-str delta))
                                :done    (println "Completed.")))
                  response  (openai/chat-completion-streaming prompt handler :sync false)]
              (println "Status:  " (:status response))
              (println "Mimetype:" (:mimetype response))
              (if (=  (:status response) 200)
                (println "Message:" (pr-str @(:data response)))
                (println "Error:"   (:data response)))))
          """ )
     :see-also '( 
          "openai/chat-completion"
          "openai/process-streaming-events" ) }

  chat-completion-streaming [prompt handler & options]

  (assert (fn? handler))

  (let [opts            (apply hash-map options)
        _               (assert (or (nil? (:prompt-opts opts)) (map? (:prompt-opts opts))))
        uri             (:uri opts "https://api.openai.com/v1/chat/completions")
        model           (:model opts default-model)
        openai-api-key  (or (:openai-api-key opts) (openai-api-key-from-env))
        _               (validate-openai-api-key openai-api-key)
        sync?           (:sync opts true)
        body            (build-prompt model prompt (:prompt-opts opts) nil {:stream true})
        debug?          (:debug opts false)
        _               (when debug? (dump-prompt body))
        response        (hc/send :post 
                          uri
                          :headers { "Content-Type" "application/json"
                                     "Authorization" (str "Bearer " openai-api-key)}
                          :body (json/write-str body)
                          :debug debug?)
        result          { :status   (:http-status response)
                          :mimetype (:content-type-mimetype response)
                          :headers  (:headers response) } ]
    (if (and (= "text/event-stream" (:content-type-mimetype response))
             (= (:http-status response) 200))
     
      (let [fr (openai/process-streaming-events response handler)]
        (assoc result :data (if sync? @fr fr)))
      (assoc result :data (hc/slurp-response response :json-parse-mode :pretty-print)))))



;; -----------------------------------------------------------------------------
;; Public utils
;; -----------------------------------------------------------------------------

(defn 
 ^{ :arglists '(
          "(process-streaming-events response handler)")
     :doc """
          Processes OpenAI server side events (SSE) and calls for every event 
          the passed handler function.
          
          Returns a `future`. This gives the caller the choice to synchronously
          or asynchronously process the events from the OpenAI server.
          
          Note: The response from the server must be of the mimetype 
                "text/event-stream" otherwise the processor throws an exception!
          
          ¶The event handler is a three argument function:
          
          `(defn handler [delta accumulated status] ...)`
          
          | *delta*       | the delta message sent with the event |
          | *accumulated* | the accumulated message |
          | *type*        | the notification type: ¶\
                            \u00A0\u00A0 `:opened` - streaming started ¶\
                            \u00A0\u00A0 `:data` - streamed event ¶\
                            \u00A0\u00A0 `:done` - streaming done by the server |
          """
     :examples '(
          """
          (do
            (load-module :openai)
            (load-module :http-client-j8 ['http-client-j8 :as 'hc])
          
            (let [api-key   (system-env "OPENAI_API_KEY")
                  content   (str "Count to 10, with a comma between each number "
                                 "and no newlines. E.g., 1, 2, 3, ...")
                  body      { :model "gpt-3.5-turbo"
                              :messages [ { :role "user"  :content content  } ]
                              :stream true } 
                  response  (hc/send :post 
                              "https://api.openai.com/v1/chat/completions"
                              :headers { "Content-Type" "application/json"
                                         "Authorization" (str "Bearer " api-key)}
                              :body (json/write-str body)
                              :debug false)]
              (println "Status:" (:http-status response))
              (if (= "text/event-stream" (:content-type-mimetype response))
                (let [text @(openai/process-streaming-events 
                                response
                                (fn [delta accumulated status]
                                  (case status
                                    :opened  (println "Started...")
                                    :data    (println "Delta:" (pr-str delta))
                                    :done    (println "Completed."))))]
                  (println "Message:" (pr-str text)))
                (println (hc/slurp-response response :json-parse-mode :pretty-print)))))
          """ )
     :see-also '( 
          "http-client-j8/slurp-response" ) }

  process-streaming-events [response handler]

  (assert (some? response))
  (assert (fn? handler))

  (when-not (= "text/event-stream" (:content-type-mimetype response))
    (throw (ex :VncException 
                """
                OpenAI server side events can only be processed on a response \
                with mimetype 'text/event-stream'!
                """)))

  ;; return a future
  (future #(do
      (let [accumulator (atom "")]
        ;; process the events
        (hc/process-server-side-events 
              response
              (fn [type event event-count]
                (case type
                  :opened (do (handler "" "" :opened)
                              :ok)
                  :data   (let [payload (first (:data event))]
                            (if (= payload "[DONE]")
                              :stop
                              (let [delta (extract-streaming-delta-content payload)]
                                (swap! accumulator str delta)
                                (handler delta @accumulator :data)
                                :ok)))
                  :closed (do (handler "" @accumulator :done)
                              :ok))))
        ;; the future returns the accumulated response text
        @accumulator))))


(defn 
  ^{ :arglists '(
          "(pretty-print-json data)")
     :doc """
          Returns a pretty printed Venice JSON data value.
          """
     :see-also '( 
          "openai/extract-response-message-content" ) }

  pretty-print-json [data]

  (if (coll? data) (json/pretty-print (json/write-str data)) data))


(defn 
  ^{ :arglists '(
          "(extract-response-message-content response)"
          "(extract-response-message-content response choice-idx)")
     :doc """
          Returns the message content of an OpenAI JSON response.
          """
     :see-also '( 
          "openai/pretty-print-json" ) }

  extract-response-message-content 
  
  ([response]
    (extract-response-message-content response 0))

  ([response choice-idx]
    (assert (coll? response))
    (-<> (:choices response)
         (nth <> choice-idx)
         (:message <>)
         (:content <>))))


(defn 
 ^{ :arglists '(
          "(extract-function-name response)"
          "(extract-function-name response choice-idx tools-calls-idx)")
     :doc """
          Returns the function name of the response.
          """
     :see-also '( 
          "openai/finish-reason"
          "openai/finish-reason-stop?"
          "openai/exec-fn" ) }

  extract-function-name 
  
  ([response]
    (extract-function-name response 0 0))

  ([response choice-idx tools-calls-idx]
    (assert finish-reason-tool-calls? response)
    (-<> (:choices response)
         (nth <> choice-idx)
         (:message  <>)
         (:tool_calls <>)
         (nth <> tools-calls-idx)
         (:function <>)
         (:name  <>))))


(defn 
  ^{ :arglists '(
          "(extract-response-message response)"
          "(extract-response-message response choice-idx)")
     :doc """
          Returns the message of an OpenAI JSON response.
          """
     :see-also '( 
          "openai/pretty-print-json" ) }

  extract-response-message 
  
  ([response]
    (extract-response-message response 0))

  ([response choice-idx]
    (assert (coll? response))
    (-<> (:choices response)
         (nth <> choice-idx)
         (:message <>))))


(defn pretty-print-conversation [messages]
  (doseq [m messages]
    (let [{:keys [role content]}  m]
      (case role
        :system     (println (ansi/style (str "system: " content) 
                                         (:bright-red ansi/ANSI-CODES)))
        :user       (println (ansi/style (str "user: " content) 
                                         (:bright-green ansi/ANSI-CODES)))
        :assistant  (println (ansi/style (str "assistant: " content) 
                                         (:bright-blue ansi/ANSI-CODES)))
        :function   (println (ansi/style (str "function: " content) 
                                         (:bright-magenta ansi/ANSI-CODES)))))))

;; -----------------------------------------------------------------------------
;; OpenAI Functions
;; -----------------------------------------------------------------------------
                                        
;; -----------------------------------------------------------------------------
;; To remember: An OpenAI responses look like
;;
;; { "created": 1714685814,
;;   "usage": {
;;     "completion_tokens": 26,
;;     "prompt_tokens": 194,
;;     "total_tokens": 220
;;   },
;;   "model": "gpt-4-0613",
;;   "id": "chatcmpl-9KYOkTLMgIEDN2hpGxCvf4cyOROHJ",
;;   "choices": [{
;;     "finish_reason": "tool_calls",
;;     "index": 0,
;;     "message": {
;;       "role": "assistant",
;;       "tool_calls": [{
;;         "function": {
;;           "name": "get_current_weather",
;;           "arguments": "{\n  \"format\": \"celsius\",\n  \"location\": \"Glasgow\"\n}"
;;         },
;;         "id": "call_I9RjSt5rnawMQD1g4GB7xGY3",
;;         "type": "function"
;;       }],
;;       "content": null
;;     },
;;     "logprobs": null
;;   }],
;;   "system_fingerprint": null,
;;   "object": "chat.completion"
;; }
;; -----------------------------------------------------------------------------


(defn 
  ^{ :arglists '(
          "(finish-reason response)"
          "(finish-reason response choice-idx)")
     :doc """
          Returns the finish reason text from an OpenAI JSON response.

          The text depends may be "stop" or "tool_calls". The first signals
          that the response contains an answer from the model to the passed
          question. With the ladder the models signals to the caller that
          functions must be executed to get specific data to answer the question.
          """
     :see-also '( 
          "openai/finish-reason-stop?"
          "openai/finish-reason-tool-calls?"
          "openai/exec-fn" ) }
  
  finish-reason 
  
  ([response]
    (finish-reason response 0))
  
  ([response choice-idx]
    (-<> (:choices response)
         (nth <> choice-idx)
         (:finish_reason <>))))


(defn 
  ^{ :arglists '(
          "(finish-reason-stop? response)"
          "(finish-reason-stop? response choice-idx)")
     :doc """
          Returns true if the OpenAI JSON response provides an answer to
          the prompt.
          """
     :see-also '( 
          "openai/finish-reason"
          "openai/finish-reason-tool-calls?"
          "openai/exec-fn" ) }

  finish-reason-stop? 
  
  ([response]
    (finish-reason-stop? response 0))

  ([response choice-idx]
    (assert map? response)
    (= "stop" (finish-reason response choice-idx))))


(defn 
  ^{ :arglists '(
          "(finish-reason-tool-calls? response)"
          "(finish-reason-tool-calls? response choice-idx)")
     :doc """
          Returns true if the OpenAI JSON response contains tool calls 
          (functions) that it wants the client to run
          """
     :see-also '( 
          "openai/finish-reason"
          "openai/finish-reason-stop?"
          "openai/exec-fn" ) }

  finish-reason-tool-calls? 
  
  ([response]
    (finish-reason-tool-calls?  response 0))

  ([response choice-idx]
    (assert map? response)
    (= "tool_calls" (finish-reason response choice-idx))))


(defn 
  ^{ :arglists '(
          "(exec-fn response fn-map)")
     :doc """
          Execute all functions from an OpenAI JSON response.

          `fn-map` is map of named functions:

          ```
          { "get_current_weather"        get-current-weather
            "get_n_day_weather_forecast" get-n-day-weather-forecast }
          ```

          Returns a list of function results, one for each function called. 

          | [![width: 15%]] | [![width: 85%]]        |
          | OK result      | `{ :ok value }`. E.g: `{ :ok "15˚C" }` |
          | ERROR result   | `{ :err exception }`. |
          """
     :see-also '( 
          "openai/finish-reason"
          "openai/finish-reason-stop?"
          "openai/finish-reason-tool-calls?" ) }

  exec-fn [response fn-map]

  (assert map? response)
  (assert map? fn-map)

  (when-not (finish-reason-tool-calls? response) 
    (throw (ex :VncEception 
              """
              The OpenAI response does not have a "tool_calls" finish reason! 
              Thus the response does not define any functions with arguments
              to call. The finish reason returned was 
              "~(exec-finish-reason response)".
              """ )))

  (loop [results []
         fns     (get-function-calls response)]
    (if (empty? fns)
      results
      (let [fn-def   (first fns) ;; next function def to process
            name     (:name fn-def)
            args-str (:arguments fn-def) ;; OpenAI decided to return a simple
                                         ;; string containing JSON data to
                                         ;; to keep the order of the args!
                                         ;; A list of tuples had been the 
                                         ;; better choice here!
            args     (json/read-str args-str)] ;; convert string to JSON
        (if-let [f (get fn-map name)]         
          (recur (conj results (call-function name f args)) ;; call the function
                 (rest fns))        
          (recur (conj results (make-err-missing-fn name)) ;; missing function
                 (rest fns)))))))


(defn- get-function-calls [response]
  (->> (:choices response)
       (first)
       (:message)
       (:tool_calls)
       (filter #(= "function" (:type %)))
       (map :function)))
  

(defn- call-function [name f args]
  (try 
    (make-ok (f args))
    (catch :Exception e (make-err-failed-call name args e))))


(defn- make-ok [ret-val]
  {:ok ret-val})


(defn- make-err-missing-fn [name]
  {:err (ex :VncEception 
            "Failed to call OpenAI undefined function \"~{name}\"!" )})
      

(defn- make-err-failed-call [name args ex_]
  (let [arity (count args)]
    {:err (ex :VncEception 
              "Failed to call OpenAI function \"~{name} with arity ~{arity}\"!" 
              ex_)}))



;; -----------------------------------------------------------------------------
;; Util functions
;; -----------------------------------------------------------------------------

(defn- extract-streaming-delta-content [payload]
  (-> (json/read-str payload :key-fn keyword)
      (:choices)
      (first)
      (:delta)
      (:content)))


(defn- openai-api-key-from-env [] 
  (system-env "OPENAI_API_KEY"))


(defn- build-prompt [model prompt prompt-opts tools tool-choice overrule-opts]
  (let [p (cond 
            (nil? prompt)        {}
            (string? prompt)     {:messages [{:role "user" :content prompt}]}
            (sequential? prompt) {:messages prompt}
            (map? prompt)        prompt
            :else                {} )]
    (merge {:model (or model default-model)} 
           p 
           (if (nil? tools) {} {:tools tools})
           (if (nil? tool-choice) {} {:tool_choice tool-choice})
           (or prompt-opts {})
           (or overrule-opts {}))))

  
(defn- dump-prompt [prompt]
  (println "\nPrompt:")
  (println (json/pretty-print (json/write-str prompt))))


(defn- validate-openai-api-key [key]
  (when (nil? key)
    (throw (ex :VncException 
               """
               Missing OpenAI api key!
               Please define an environment variable with the name \
               "OPENAI_API_KEY" or pass the key as an option \
               `:openai-api-key "sk-xxxxxxxxxxxxx"`.
               """))))


(defn- validate-tools [tools]
  (when (some? tools)
    (when-not (sequential? tools)
      (throw (ex :VncException 
                  """
                  OpenAI tools options (`:tools`) must be `nil` or a sequence of maps!

                  E.g.:

                  [ { :type "function" 
                      :function {
                          :name "get_current_weather"
                          :description "Get the current weather"
                          :parameters { ... }
                      } } ]
                  """)))))


(defn- validate-tool-choice [tools-choice]
  (when (some? tools-choice)
    (when-not (or (string? tools-choice) (map? tools-choice))
      (throw (ex :VncException 
                  """
                  An OpenAI tool choice option (`:tool_choice`) must be `nil` 
                  a string or a map!

                  E.g.:

                  { :type "function"
                    :function { 
                      :name "get_n_day_weather_forecast" 
                    } 
                  }
                  """)))))
